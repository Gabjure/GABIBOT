{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            GENERERATIVE CHATBOT MADE w/ KERAS, RNN (LSTM) AND TRAINED w/ SARC DS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind this documents the code, some parts might not be in the correct order, for the working code please go to the XXX document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REQUIREMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:17:24.295255Z",
     "start_time": "2020-06-02T14:17:24.283448Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Basic libraries to import:\n",
    "import numpy as np  #used for scientific computing\n",
    "import pandas as pd #for data manipulation and analysis - used to upload de DS we are working with.\n",
    "\n",
    "#NLP\n",
    "import nltk # Natural Language Toolkit, platform for building Python programs to work with human language data.\n",
    "\n",
    "#nltk.download('punkt') # tokenizer that divides a text into a list of sentences\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model \n",
    "\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:18:54.317661Z",
     "start_time": "2020-06-02T12:18:54.297560Z"
    }
   },
   "outputs": [],
   "source": [
    "# set default parameters\n",
    "BATCH_SIZE = 128 # number of samples processed before the model is updated. \n",
    "NUM_EPOCHS = 500 # number of complete passes through the training dataset. For generative models we need a lot of epochs so the model can learn, we will see if 500 is enough\n",
    "HIDDEN_UNITS = 100 #number of hidden layers, they perform nonlinear transformations of the inputs entered into the network.\n",
    "MAX_INPUT_SEQ_LENGTH = 20 # max. number of words the chatbot will consider as input\n",
    "MAX_TARGET_SEQ_LENGTH = 20 # max. number of words the chatbot will reply with\n",
    "MAX_VOCAB_SIZE = 20000 #10-20k  https://coursefinders.com/blog/es/5669/espanol-cuantas-palabras-se-necesitan-para-hablar-con-fluidez-un-idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.199213Z",
     "start_time": "2020-06-02T12:18:54.999582Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('SARC_DS.csv')\n",
    "lines = df['all']\n",
    "\n",
    "# Containers that keeps track of how many times equivalent values are added.\n",
    "# Used for the text preprocessing and the encoding process\n",
    "from collections import Counter \n",
    "\n",
    "input_counter = Counter()\n",
    "target_counter = Counter()\n",
    "\n",
    "#create the vocabulary from the dataset to train the model\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "prev_words = []\n",
    "\n",
    "import nltk \n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    next_words = [w.lower() for w in nltk.word_tokenize(line) if w.isalpha()]\n",
    "\n",
    "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
    "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
    "\n",
    "    if len(prev_words) > 0:\n",
    "        input_texts.append(prev_words)\n",
    "        \n",
    "        for w in prev_words:\n",
    "            input_counter[w] += 1\n",
    "            \n",
    "        target_words = next_words[:]\n",
    "        target_words.insert(0, 'START')\n",
    "        target_words.append('END')\n",
    "        \n",
    "        for w in target_words:\n",
    "            target_counter[w] += 1\n",
    "            \n",
    "        target_texts.append(target_words)\n",
    "\n",
    "    prev_words = next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the data\n",
    "\n",
    "input_word2idx = dict()\n",
    "target_word2idx = dict()\n",
    "\n",
    "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    input_word2idx[word[0]] = idx + 2\n",
    "    \n",
    "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    target_word2idx[word[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('input_word2idx.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    input_word2idx = pickle.load(f)\n",
    "\n",
    "with open('target_word2idx.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    target_word2idx = pickle.load(f)\n",
    "\n",
    "input_word2idx['PAD'] = 0\n",
    "input_word2idx['UNK'] = 1\n",
    "target_word2idx['UNK'] = 0\n",
    "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_encoder_tokens = len(input_idx2word)\n",
    "num_decoder_tokens = len(target_idx2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.203212Z",
     "start_time": "2020-06-02T12:18:56.084Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_data = []\n",
    "\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "for input_words, target_words in zip(input_texts, target_texts):\n",
    "    encoder_input_wids = []\n",
    "    \n",
    "    for w in input_words:\n",
    "        w2idx = 1\n",
    "        \n",
    "        if w in input_word2idx:\n",
    "            w2idx = input_word2idx[w]\n",
    "            \n",
    "        encoder_input_wids.append(w2idx)\n",
    "\n",
    "    encoder_input_data.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
    "\n",
    "    \n",
    "context = dict()\n",
    "context['num_encoder_tokens'] = num_encoder_tokens\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.208513Z",
     "start_time": "2020-06-02T12:18:57.517Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(input_data, output_text_data, BATCH_SIZE):\n",
    "    '''\n",
    "    Custom function to generate batches\n",
    "    \n",
    "    input: \n",
    "        - input_data \n",
    "        - output_text_data\n",
    "        - BATCH_SIZE\n",
    "        \n",
    "    output:\n",
    "        - generator object\n",
    "    '''\n",
    "    \n",
    "    num_batches = len(input_data) // BATCH_SIZE\n",
    "    \n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            \n",
    "            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)\n",
    "            \n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            \n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0\n",
    "                    \n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                    \n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            \n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.Input()\n",
    "It's used to instantiate a Keras tensor; a TensorFlow symbolic tensor object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model. https://keras.io/api/layers/core_layers/input/\n",
    "        \n",
    "    Used Arguments:\n",
    "- **shape**: shape tuple (integers), not including the batch size. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known.<br>\n",
    "- **name**: String, the name of the layer. (opt)\n",
    "<br>\n",
    "\n",
    "\n",
    "    Returns:\n",
    "- Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.211880Z",
     "start_time": "2020-06-02T12:19:00.069Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "encoder_inputs = Input(shape=(None,), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), \n",
    "                        Input(shape=(HIDDEN_UNITS,))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.Embedding()\n",
    "Turns positive integers (indexes) into dense vectors of fixed size. This layer can only be used as the first layer in a model. https://keras.io/api/layers/core_layers/embedding/\n",
    "        \n",
    "    Used Arguments:\n",
    "- **input_dim**: Integer. Size of the vocabulary, i.e. maximum integer index + 1.<br>\n",
    "- **output_dim**: Integer. Dimension of the dense embedding.<br>\n",
    "- **input_length**: Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).<br>\n",
    "- **name**: String, the name of the layer. (opt)\n",
    "<br>\n",
    "\n",
    "\n",
    "    Input shape:\n",
    "- 2D tensor with shape: (batch_size, input_length).\n",
    "<br>\n",
    "\n",
    "\n",
    "    Output shape:    \n",
    "- 3D tensor with shape: (batch_size, input_length, output_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, \n",
    "                              output_dim=HIDDEN_UNITS,\n",
    "                              input_length=encoder_max_seq_length, \n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.LSTM()\n",
    "*(Long Short-Term Memory layer - Hochreiter 1997.)*\n",
    "Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "        \n",
    "    Used Arguments:\n",
    "    \n",
    "- **units**: Positive integer, dimensionality of the output space.<br>\n",
    "- **return_sequences**: Boolean. Whether to return the last output. in the output sequence, or the full sequence. Default: 'False'.<br>\n",
    "- **return_state**: Boolean. Whether to return the last state in addition to the output. Default: 'False'.<br>\n",
    "- **name**: String, the name of the layer. (opt) <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "    Used Called Arguments:\n",
    "    \n",
    "- **initial_state**: List of initial state tensors to be passed to the first call of the cell (optional, defaults to None which causes creation of zero-filled initial state tensors).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.215393Z",
     "start_time": "2020-06-02T12:19:03.746Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, \n",
    "                    return_state=True, \n",
    "                    name='encoder_lstm')\n",
    "\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(\n",
    "                                                encoder_embedding(\n",
    "                                                    encoder_inputs))\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, \n",
    "                    return_state=True, \n",
    "                    return_sequences=True, \n",
    "                    name='decoder_lstm')\n",
    "\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(\n",
    "                                        decoder_inputs,\n",
    "                                        initial_state=encoder_states)\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, \n",
    "                                                 initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.layers.Dense()\n",
    "Dense layer is the regular deeply connected neural network layer. https://keras.io/api/layers/core_layers/dense/\n",
    "        \n",
    "    Arguments\n",
    "- **units**: Number of units and it affects the output layer.<br>\n",
    "- **activation**: Activation function. <br>\n",
    "- **name**: String, the name of the layer. (opt)\n",
    "<br>\n",
    "\n",
    "\n",
    "    Input shape:\n",
    "- N-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).\n",
    "<br>\n",
    "\n",
    "\n",
    "    Output shape:    \n",
    "- N-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T12:19:16.218287Z",
     "start_time": "2020-06-02T12:19:10.183Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "decoder_dense = Dense(\n",
    "                units=num_decoder_tokens,\n",
    "                activation='softmax', #converts a real vector to a vector of categorical probabilities\n",
    "                name='decoder_dense'\n",
    "                )\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBJECT CREATION & MODEL CONFIGURATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.Model()\n",
    "Model groups layers into an object with training and inference features. https://keras.io/api/models/model/#model-class\n",
    "\n",
    "    Arguments\n",
    "- **inputs**: The input(s) of the model: a `keras.Input` object or list of `keras.Input` objects. <br>\n",
    "- **outputs**: The output(s) of the model. <br>\n",
    "- **name**: String, the name of the model. (opt) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T20:30:31.078129Z",
     "start_time": "2020-05-31T20:30:31.058217Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model,\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, \n",
    "                      [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.model.compile()\n",
    "To configurate the model with loses and optimizers, by default. You can also add metrics\n",
    "    \n",
    "    Losses: \n",
    "\n",
    "The purpose is to compute the quantity that a model should seek to minimize during training.\n",
    "https://keras.io/api/losses/<br>\n",
    "\n",
    "- **CategoricalCrossentropy**: Default. Computes the cross entropy loss between the labels and predictions; when there are two or more label classes.\n",
    "<br>\n",
    "       \n",
    "       \n",
    "    Optimizers: \n",
    "\n",
    "An optimizer is one of the two arguments required for compiling a Keras model, it's objective... optimize: https://keras.io/api/optimizers/<br>\n",
    "\n",
    "- **Adam**: Default. Stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. /it works for generative models because according to Kingma et al., 2014, 'is well suited for problems that are large in terms of data'/\n",
    "<br>\n",
    "    \n",
    "    \n",
    "    Metrics: /not used/\n",
    "\n",
    "Used to judge the performance of your model.\n",
    "https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T16:56:24.414834Z",
     "start_time": "2020-05-31T16:56:24.299876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "#Compiling\n",
    "#training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "#training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING PREP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sklearn.model_selection.train_test_split()\n",
    "\n",
    "Split arrays or matrices into random train and test subsets. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    \n",
    "    Used Parameters: \n",
    "\n",
    "- ***arrays**: sequence of indexables with same length.<br>\n",
    "- ***test_size**: proportion of the dataset to include in the test split.<br>\n",
    "- ***train_size**: proportion of the dataset to include in the train split.<br>\n",
    "- ***random_state**: parameter to control the random number generator used. /For more information about random_state check out: https://scikit-learn.org/stable/glossary.html#term-random-state /\n",
    "<br>\n",
    "       \n",
    "       \n",
    "    Returns: \n",
    "\n",
    "- **splitting**: list, length=2 * len(arrays): X_train, X_test, y_train, y_test /Careful with the order/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T08:55:43.795718Z",
     "start_time": "2020-05-31T08:55:43.685212Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, \n",
    "                                                    target_texts, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, BATCH_SIZE)\n",
    "test_gen = generate_batch(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_num_batches = len(X_train) // BATCH_SIZE\n",
    "test_num_batches = len(X_test) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras.callbacks.ModelCheckpoint()\n",
    "\n",
    "Callback to save the Keras model or model weights at some frequency. https://keras.io/api/callbacks/model_checkpoint/\n",
    "    \n",
    "    Used Arguments: \n",
    "\n",
    "- **filepath**: string or PathLike, path to save the model file.<br>\n",
    "- **monitor**: quantity to monitor.<br>\n",
    "- **verbose**:verbosity mode, 0 or 1.<br>\n",
    "- **save_best_only**:when save_best_only=True, the latest best model according to the quantity monitored will not be overwritten. <br>\n",
    "- **mode**: For 'loss' this should be min.<br>\n",
    "- ****kwargs**: Additional arguments for backwards compatibility. Possible key is period.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T14:17:42.891444Z",
     "start_time": "2020-06-02T14:17:42.884421Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "                            'model_best_weights', #name of the document where the checkpoints will be saved\n",
    "                            monitor='loss', \n",
    "                            verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            mode='min', \n",
    "                            period = 3 #saved every 3 epochs when loss improves\n",
    "                            )\n",
    "\n",
    "my_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T23:02:07.509236Z",
     "start_time": "2020-05-30T23:02:07.499368Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "######\n",
    "To train my model with the established vocabulary of 20k words and 500 epochs; and starting with 40k lines of my ds it would take an estimate of 17 days to train in my local machine.\n",
    "\n",
    "Epoch 1/500\n",
    "  8/312 [..............................] - ETA: 53:45 - loss: 9.0315\n",
    "  \n",
    " Using a VM the same model with the same parameters takes an estimate of 5 days.\n",
    " \n",
    " Epoch 1/500\n",
    "249/249 [==============================] - 923s 4s/step - loss: 6.7226 - val_loss: 6.4012\n",
    "\n",
    "To avoid unnecessary training I am using the callback attribute with `Earlystop` which will automatically stop the training once it has stopped improving, this means that it might take less time. We will see, today is 31.May.2020 01:19-\n",
    "\n",
    "-- **`40k lines of training is insufficient to have a properly trained model`**\n",
    "\n",
    "At the same time, given that it takes days to train, to avoid any technical issues I am using the `ModelCheckpoint` function which saves a copy of the model, in this case it is monitoring the 'loss', which on second though might not be the correct one and it should be monitoring the 'val_loss', but the main thing is that it makes a \"Security Copy\" of the model so if the Server of the VM were to disconnect or the Kernel were to die I'd have a least part of the model saved.\n",
    "\n",
    "Because we are working with NLG the training has to be very extense. That's why I'm creating different models, which might not be completed for due date but will be useful to get a general idea and even continue training that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fit_generator()\n",
    "\n",
    "**Model.fit now supports generators, so there is no longer any need to use this endpoint.**\n",
    "\n",
    "Fits the model on data yielded batch-by-batch by a Python generator. https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit // https://kite.com/python/docs/tensorflow.keras.Model.fit_generator\n",
    "    \n",
    "    Used Arguments: \n",
    "\n",
    "- **generator**: A generator object, created from a tuple `(inputs, targets)`, in order to avoid duplicate data when using multiprocessing.<br>\n",
    "- **steps_per_epoch**: Total number of steps (batches of samples) to yield from `generator` before declaring one epoch finished and starting the next epoch.<br>\n",
    "- **epochs**: Integer, total number of iterations on the data.<br>\n",
    "- **verbose**:Verbosity mode, 0, 1, or 2; used for each epochs progress visualization <br>\n",
    "- **callbacks**: List of callbacks to be called during training.<br>\n",
    "- **validation_data**: generator for the validation data.<br>\n",
    "- **validation_steps**:Total number of steps (batches of samples) to yield from `generator` before stopping.\n",
    "<br>\n",
    "    \n",
    "    \n",
    "    Retruns:\n",
    "\n",
    "\n",
    "- A `History` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T22:52:02.691316Z",
     "start_time": "2020-05-30T22:52:02.685554Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_tot_500e = model.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_num_batches,\n",
    "                    epochs= NUM_EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_gen,\n",
    "                    validation_steps=test_num_batches,\n",
    "                    callbacks = my_callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING & LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T22:53:46.630452Z",
     "start_time": "2020-05-30T22:53:46.624981Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model.save(\"file_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:09:23.733193Z",
     "start_time": "2020-05-31T09:09:21.267197Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('file_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:51:36.070585Z",
     "start_time": "2020-05-31T09:51:34.570383Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = input()\n",
    "input_seq = []\n",
    "input_wids = []\n",
    "max_encoder_seq_length = 20\n",
    "max_decoder_seq_length = 20\n",
    "\n",
    "for word in nltk.word_tokenize(input_text.lower()):\n",
    "    idx = 1\n",
    "    if word in input_word2idx:\n",
    "        idx = input_word2idx[word]\n",
    "    input_wids.append(idx)\n",
    "    \n",
    "input_seq.append(input_wids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pad_sequence()\n",
    "\n",
    "Pads sequences to the same length, in other words, the function transforms a list of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, maxlen). \n",
    "https://keras.io/api/preprocessing/timeseries/#pad_sequences-function\n",
    "    \n",
    "    Used Arguments: \n",
    "\n",
    "- **sequences**: List of sequences (each sequence is a list of integers).<br>\n",
    "- **maxlen**: Optional Int, maximum length of all sequences. If not provided, sequences will be padded to the length of the longest individual sequence.\n",
    "<br>\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "\n",
    "- Numpy array with shape (len(sequences), maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:51:36.070585Z",
     "start_time": "2020-05-31T09:51:34.570383Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_seq = pad_sequences(input_seq, \n",
    "                          maxlen= max_encoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T09:51:36.070585Z",
     "start_time": "2020-05-31T09:51:34.570383Z"
    }
   },
   "outputs": [],
   "source": [
    "states_value = encoder_model.predict(input_seq)\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, target_word2idx['START']] = 1\n",
    "target_text = ''\n",
    "target_text_len = 0\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "    sample_word = target_idx2word[sample_token_idx]\n",
    "    target_text_len += 1\n",
    "\n",
    "    if sample_word != 'START' and sample_word != 'END':\n",
    "        target_text += ' ' + sample_word\n",
    "\n",
    "    if sample_word == 'END' or target_text_len >= max_decoder_seq_length:\n",
    "        terminated = True\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "    states_value = [h, c]\n",
    "    \n",
    "\n",
    "target_text.strip().replace('UNK', '')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            GENERERATIVE CHATBOT MADE w/ KERAS, RNN (LSTM) AND TRAINED w/ SARC DS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T22:02:44.516637Z",
     "start_time": "2020-05-30T22:02:44.457287Z"
    }
   },
   "outputs": [],
   "source": [
    "#Basic libraries to import:\n",
    "import numpy as np  #used for scientific computing\n",
    "import pandas as pd #for data manipulation and analysis - used to upload de DS we are working with.\n",
    "import pickle\n",
    "\n",
    "#NLP\n",
    "import nltk # Natural Language Toolkit, platform for building Python programs to work with human language data.\n",
    "\n",
    "#nltk.download('punkt') # tokenizer that divides a text into a list of sentences\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model \n",
    "\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_batch(input_data, output_text_data, BATCH_SIZE):\n",
    "    '''\n",
    "    Custom function to generate batches\n",
    "    \n",
    "    input: \n",
    "        - input_data \n",
    "        - output_text_data\n",
    "        - BATCH_SIZE\n",
    "        \n",
    "    output:\n",
    "        - generator object\n",
    "    '''\n",
    "    \n",
    "    num_batches = len(input_data) // BATCH_SIZE\n",
    "    \n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            \n",
    "            encoder_input_data_batch = pad_sequences(input_data[start:end], encoder_max_seq_length)\n",
    "            \n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens))\n",
    "            \n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0\n",
    "                    \n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                    \n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            \n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T22:02:44.516637Z",
     "start_time": "2020-05-30T22:02:44.457287Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "# set default parameters\n",
    "BATCH_SIZE = 128 # number of samples processed before the model is updated. \n",
    "NUM_EPOCHS = 500 # number of complete passes through the training dataset.\n",
    "HIDDEN_UNITS = 100 #number of hidden layers, they perform nonlinear transformations of the inputs entered into the network.\n",
    "MAX_INPUT_SEQ_LENGTH = 20 # max. number of words the chatbot will consider as input\n",
    "MAX_TARGET_SEQ_LENGTH = 20 # max. number of words the chatbot will reply with\n",
    "MAX_VOCAB_SIZE = 20000 #10-20k  https://coursefinders.com/blog/es/5669/espanol-cuantas-palabras-se-necesitan-para-hablar-con-fluidez-un-idioma\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv('SARC_DS.csv')\n",
    "lines = df['all']\n",
    "\n",
    "# Containers that keeps track of how many times equivalent values are added.\n",
    "input_counter = Counter()\n",
    "target_counter = Counter()\n",
    "\n",
    "#create the vocabulary from the dataset to train the model\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "prev_words = []\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    next_words = [w.lower() for w in nltk.word_tokenize(line) if w.isalpha()]\n",
    "\n",
    "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
    "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
    "\n",
    "    if len(prev_words) > 0:\n",
    "        input_texts.append(prev_words)\n",
    "        \n",
    "        for w in prev_words:\n",
    "            input_counter[w] += 1\n",
    "            \n",
    "        target_words = next_words[:]\n",
    "        target_words.insert(0, 'START')\n",
    "        target_words.append('END')\n",
    "        \n",
    "        for w in target_words:\n",
    "            target_counter[w] += 1\n",
    "            \n",
    "        target_texts.append(target_words)\n",
    "\n",
    "    prev_words = next_words\n",
    "\n",
    "# encode the data\n",
    "\n",
    "input_word2idx = dict()\n",
    "target_word2idx = dict()\n",
    "\n",
    "\n",
    "for idx, word in enumerate(input_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    input_word2idx[word[0]] = idx + 2\n",
    "    \n",
    "for idx, word in enumerate(target_counter.most_common(MAX_VOCAB_SIZE)):\n",
    "    target_word2idx[word[0]] = idx + 1\n",
    "\n",
    "input_word2idx['PAD'] = 0\n",
    "input_word2idx['UNK'] = 1\n",
    "target_word2idx['UNK'] = 0\n",
    "\n",
    "input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_encoder_tokens = len(input_idx2word)\n",
    "num_decoder_tokens = len(target_idx2word)\n",
    "\n",
    "\n",
    "encoder_input_data = []\n",
    "\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "for input_words, target_words in zip(input_texts, target_texts):\n",
    "    encoder_input_wids = []\n",
    "    \n",
    "    for w in input_words:\n",
    "        w2idx = 1\n",
    "        \n",
    "        if w in input_word2idx:\n",
    "            w2idx = input_word2idx[w]\n",
    "            \n",
    "        encoder_input_wids.append(w2idx)\n",
    "\n",
    "    encoder_input_data.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
    "\n",
    "    \n",
    "context = dict()\n",
    "context['num_encoder_tokens'] = num_encoder_tokens\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
    "\n",
    "\n",
    "# input layer creation\n",
    "encoder_inputs = Input(shape=(None,), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "# embedding layer creation\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, \n",
    "                              output_dim=HIDDEN_UNITS,\n",
    "                              input_length=encoder_max_seq_length, \n",
    "                              name='encoder_embedding')\n",
    "\n",
    "# LSTM layer creation\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, \n",
    "                    return_state=True, \n",
    "                    name='encoder_lstm')\n",
    "\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(\n",
    "                                                encoder_embedding(\n",
    "                                                    encoder_inputs))\n",
    "\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, \n",
    "                    return_state=True, \n",
    "                    return_sequences=True, \n",
    "                    name='decoder_lstm')\n",
    "\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(\n",
    "                                        decoder_inputs,\n",
    "                                        initial_state=encoder_states)\n",
    "\n",
    "# Dense layer creation\n",
    "decoder_dense = Dense(\n",
    "                units=num_decoder_tokens,\n",
    "                activation='softmax', #converts a real vector to a vector of categorical probabilities\n",
    "                name='decoder_dense'\n",
    "                )\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# model creation\n",
    "model = Model([encoder_inputs, decoder_inputs], \n",
    "               decoder_outputs)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# spliting and training preparation\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoder_input_data, \n",
    "                                                    target_texts, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, BATCH_SIZE)\n",
    "test_gen = generate_batch(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_num_batches = len(X_train) // BATCH_SIZE\n",
    "test_num_batches = len(X_test) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model_best_weights', #name of the document where the checkpoints will be saved\n",
    "                            monitor='loss', \n",
    "                            verbose=1, \n",
    "                            save_best_only=True, \n",
    "                            mode='min', \n",
    "                            period= 1) #saved every <INT> epochs when loss improves\n",
    "                            \n",
    "\n",
    "my_callbacks = [checkpoint]\n",
    "\n",
    "model.summary() #shows models layers & summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T22:06:41.299823Z",
     "start_time": "2020-05-30T22:03:13.054854Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"name_of_file\")\n",
    "fitted_model = model.fit_generator(generator=train_gen,\n",
    "                    steps_per_epoch=train_num_batches,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_gen,\n",
    "                    validation_steps=test_num_batches,\n",
    "                    callbacks = my_callbacks\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('name_of_file2') # to use a saved model load it.\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We test the models to see if they work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = input()\n",
    "input_seq = []\n",
    "input_wids = []\n",
    "max_encoder_seq_length = 10\n",
    "max_decoder_seq_length = 10\n",
    "\n",
    "for word in nltk.word_tokenize(input_text.lower()):\n",
    "    idx = 1\n",
    "    if word in input_word2idx:\n",
    "        idx = input_word2idx[word]\n",
    "    input_wids.append(idx)\n",
    "    \n",
    "input_seq.append(input_wids)\n",
    "input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, target_word2idx['START']] = 1\n",
    "target_text = ''\n",
    "target_text_len = 0\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "    sample_word = target_idx2word[sample_token_idx]\n",
    "    target_text_len += 1\n",
    "\n",
    "    if sample_word != 'START' and sample_word != 'END':\n",
    "        target_text += ' ' + sample_word\n",
    "\n",
    "    if sample_word == 'END' or target_text_len >= max_decoder_seq_length:\n",
    "        terminated = True\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "    states_value = [h, c]\n",
    "    \n",
    "\n",
    "target_text.strip().replace('UNK', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"name_of_file2\") #saved to .h5 no need to add extension\n",
    "\n",
    "encoder_model.save('encoder_model_500e_2kl')\n",
    "decoder_model.save('decoder_model_500e_2kl')\n",
    "\n",
    "\n",
    "with open('target_word2idx_500e_2kl.pickle', 'wb') as handle:\n",
    "    pickle.dump(target_word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('input_word2idx_500e_2kl.pickle', 'wb') as handle:\n",
    "    pickle.dump(input_word2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

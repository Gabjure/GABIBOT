{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:20.966053Z",
     "start_time": "2020-05-22T08:25:20.962296Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip3 install 'tensorflow==1.13.0rc1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.435561Z",
     "start_time": "2020-05-22T08:25:20.969582Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import helpers\n",
    "\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.550491Z",
     "start_time": "2020-05-22T08:25:25.438797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Load Data\n",
    "header = ['Context', 'Utterance', 'Label']\n",
    "train_df = pd.read_csv(\"data/trainset.csv\", names = header, nrows = 10000)\n",
    "test_df = pd.read_csv(\"data/testset.csv\", names = header, nrows = 3000)\n",
    "validation_df = pd.read_csv(\"data/valset.csv\", names = header, nrows = 3000)\n",
    "y_test = np.zeros(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline models for expected performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.562106Z",
     "start_time": "2020-05-22T08:25:25.553509Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_recall(y, y_test, k=1):\n",
    "    '''\n",
    "    Evaluates recall@k (model picks k best responses out of 10). \n",
    "        If correct one is choosen test is marked as correct\n",
    "    \n",
    "    input:\n",
    "        y: list of our predictions sorted by score in descending order\n",
    "        y_test: label\n",
    "        k: number of responses the model can pick (10 = recall of 100%)\n",
    "    \n",
    "    '''\n",
    "    num_examples = float(len(y))\n",
    "    num_correct = 0\n",
    "    for predictions, label in zip(y, y_test):\n",
    "        if label in predictions[:k]:\n",
    "            num_correct += 1\n",
    "    return num_correct/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.573251Z",
     "start_time": "2020-05-22T08:25:25.567465Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Random Predictor - \n",
    "# def predict_random(context, utterances):\n",
    "#     return np.random.choice(len(utterances), 10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.594564Z",
     "start_time": "2020-05-22T08:25:25.583464Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Evaluate Random predictor \n",
    "# # to make sure the evaluate_recall function works properly\n",
    "\n",
    "# y_random = [predict_random(test_df.Context[x], test_df.iloc[x,1:].values) for x in range(len(test_df))]\n",
    "# y_test = np.zeros(len(y_random))\n",
    "# for n in [1, 2, 5, 10]:\n",
    "#     print('Recall @ ({}, 10): {:g}'.format(n, evaluate_recall(y_random, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline models for TF-IDF predictor and expected performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.619437Z",
     "start_time": "2020-05-22T08:25:25.602941Z"
    }
   },
   "outputs": [],
   "source": [
    "class TFIDFPredictor:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    " \n",
    "    def train(self, data):\n",
    "        self.vectorizer.fit(np.append(data.Context.values,data.Utterance.values))\n",
    " \n",
    "    def predict(self, context, utterances):\n",
    "        # Convert context and utterances into tfidf vector\n",
    "        vector_context = self.vectorizer.transform({context})\n",
    "        vector_doc = self.vectorizer.transform(utterances)\n",
    "        # The dot product measures the similarity of the resulting vectors\n",
    "        result = np.dot(vector_doc, vector_context.T).todense()\n",
    "        result = np.asarray(result).flatten()\n",
    "        # Sort by top results and return the indices in descending order\n",
    "        return np.argsort(result, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:25:25.664458Z",
     "start_time": "2020-05-22T08:25:25.626488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey all, if i enable nvidia-glx on my system, the fonts in KDE get smaller ! ? strange, although the size hasn't changed </s> do you have a higher resolution or is it just small font </s>  it's the same resolution. The fonts in Opera don't change, it only happens in KDE </s> yes </s> i mean yes there is another way </s> you need to edit your fstab </s>  i know, but what can i write there ? </s> in the options section instead of defaults write somthing like this. rw,user     <- user makes it so anyone can mount it with full rw permissions\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:05.880242Z",
     "start_time": "2020-05-22T08:25:25.668323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ (1, 10): 0.0123333\n",
      "Recall @ (2, 10): 0.016\n",
      "Recall @ (5, 10): 0.0313333\n",
      "Recall @ (10, 10): 0.0443333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate TFIDF predictor ###FIXED\n",
    "pred = TFIDFPredictor()\n",
    "pred.train(train_df)\n",
    "y = [pred.predict(test_df.Context[x], test_df.Utterance[x:]) for x in range(len(test_df))]\n",
    "for n in [1, 2, 5, 10]:\n",
    "    print('Recall @ ({}, 10): {:g}'.format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Encoder LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE - seq2seq model would be another option instead of Dual Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.094689Z",
     "start_time": "2020-05-22T08:27:05.882892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from absl-py) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.106740Z",
     "start_time": "2020-05-22T08:27:10.098813Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.116785Z",
     "start_time": "2020-05-22T08:27:10.111672Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.139463Z",
     "start_time": "2020-05-22T08:27:10.119756Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(hparams):\n",
    "    '''\n",
    "    We obtain vector representation for words.\n",
    "    input: \n",
    "        hyperparameters\n",
    "    output:\n",
    "        Gets an existing variable with these parameters or create a new one\n",
    "        \n",
    "    '''\n",
    "    if hparams.glove_path and hparams.vocab_path:\n",
    "        tf.logging.info(\"Loading Glove embeddings...\")\n",
    "        vocab_array, vocab_dict = helpers.load_vocab(hparams.vocab_path)\n",
    "        glove_vectors, glove_dict = helpers.load_glove_vectors(hparams.glove_path, vocab=set(vocab_array))\n",
    "        initializer = helpers.build_initial_embedding_matrix(vocab_dict, glove_dict, glove_vectors, hparams.embedding_dim)\n",
    "    else:\n",
    "        tf.logging.info(\"No glove/vocab path specificed, starting with random embeddings.\")\n",
    "        initializer = tf.random_uniform_initializer(-0.25, 0.25)\n",
    "    return tf.compat.v1.get_variable(\n",
    "        \"word_embeddings\",\n",
    "        shape=[hparams.vocab_size, hparams.embedding_dim],\n",
    "        initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.185812Z",
     "start_time": "2020-05-22T08:27:10.154191Z"
    }
   },
   "outputs": [],
   "source": [
    "def dual_encoder_model(\n",
    "    hparams, \n",
    "    mode, \n",
    "    context,\n",
    "    context_len,\n",
    "    utterance,\n",
    "    utterance_len,\n",
    "    targets):\n",
    "    \n",
    "    # Initialize embedidngs randomly or with pre-trained vectors if available\n",
    "    embeddings_W = get_embeddings(hparams)\n",
    "\n",
    "    # Embed the context and the utterance\n",
    "    context_embedded = tf.nn.embedding_lookup(\n",
    "        embeddings_W, context, name=\"embed_context\")\n",
    "    utterance_embedded = tf.nn.embedding_lookup(\n",
    "        embeddings_W, utterance, name=\"embed_utterance\")\n",
    "\n",
    "    # Build the RNN\n",
    "    with tf.variable_scope(\"rnn\") as vs:\n",
    "        # We use an LSTM Cell\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(\n",
    "            hparams.rnn_dim,\n",
    "            forget_bias=2.0,\n",
    "            use_peepholes=True,\n",
    "            state_is_tuple=True)\n",
    "\n",
    "        # Run the utterance and context through the RNN\n",
    "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            tf.concat(0, [context_embedded, utterance_embedded]),\n",
    "            sequence_length=tf.concat(0, [context_len, utterance_len]),\n",
    "            dtype=tf.float32)\n",
    "        encoding_context, encoding_utterance = tf.split(0, 2, rnn_states.h)\n",
    "\n",
    "    with tf.variable_scope(\"prediction\") as vs:\n",
    "        M = tf.get_variable(\"M\", \n",
    "            shape=[hparams.rnn_dim, hparams.rnn_dim],\n",
    "            initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "        # \"Predict\" a  response: c * M\n",
    "        generated_response = tf.matmul(encoding_context, M)\n",
    "        generated_response = tf.expand_dims(generated_response, 2)\n",
    "        encoding_utterance = tf.expand_dims(encoding_utterance, 2)\n",
    "\n",
    "        # Dot product between generated response and actual response\n",
    "        # (c * M) * r\n",
    "        logits = tf.batch_matmul(generated_response, encoding_utterance, True)\n",
    "        logits = tf.squeeze(logits, [2])\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probs = tf.sigmoid(logits)\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            return probs, None\n",
    "\n",
    "        # Calculate the binary cross-entropy loss\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(logits, tf.to_float(targets))\n",
    "\n",
    "    # Mean loss across the batch of examples\n",
    "    mean_loss = tf.reduce_mean(losses, name=\"mean_loss\")\n",
    "    return probs, mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE - There’s also tf.SequenceExample but it doesn’t seem to be supported by tf.learn yet (2016) - \n",
    "\n",
    "\n",
    "Esto sirve en caso de encontrar un DataSet con el que entrenar, preguntar mañana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.201384Z",
     "start_time": "2020-05-22T08:27:10.189598Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.218479Z",
     "start_time": "2020-05-22T08:27:10.206522Z"
    }
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.compat.v1.flags.FLAGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.235003Z",
     "start_time": "2020-05-22T08:27:10.222483Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "  \"min_word_frequency\", 5, \"Minimum frequency of words in the vocabulary\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"max_sentence_len\", 160, \"Maximum Sentence Length\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "  \"input_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Input directory containing original CSV data files (default = './data')\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "  \"output_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Output directory for TFrEcord files (default = './data')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.249148Z",
     "start_time": "2020-05-22T08:27:10.239187Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys #fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.268888Z",
     "start_time": "2020-05-22T08:27:10.262907Z"
    }
   },
   "outputs": [],
   "source": [
    "remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")]) #fix\n",
    "assert(remaining_args == [sys.argv[0]]) #fix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.280177Z",
     "start_time": "2020-05-22T08:27:10.274865Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer_fn(iterator):\n",
    "    return (x.split(\" \") for x in iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.290254Z",
     "start_time": "2020-05-22T08:27:10.284280Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_csv_iter(filename):\n",
    "    \"\"\"\n",
    "    Returns an iterator over a CSV file. Skips the header.\n",
    "    \"\"\"\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        # Skip the header\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.300682Z",
     "start_time": "2020-05-22T08:27:10.295262Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vocab(input_iter, min_frequency):\n",
    "    \"\"\"\n",
    "    Creates and returns a VocabularyProcessor object with the vocabulary\n",
    "    for the input iterator.\n",
    "    \"\"\"\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      FLAGS.max_sentence_len,\n",
    "      min_frequency=min_frequency,\n",
    "      tokenizer_fn=tokenizer_fn)\n",
    "    return vocab_processor.fit(input_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.313082Z",
     "start_time": "2020-05-22T08:27:10.305184Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def transform_sentence(sequence, vocab_processor):\n",
    "    \"\"\"\n",
    "    Maps a single sentence into the integer vocabulary. \n",
    "    Returns a python array.\n",
    "    \"\"\"\n",
    "    return next(vocab_processor.transform([sequence])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.323122Z",
     "start_time": "2020-05-22T08:27:10.316741Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_text_sequence_feature(fl, sentence, sentence_len, vocab):\n",
    "    \"\"\"\n",
    "    Writes a sentence to FeatureList protocol buffer\n",
    "    \"\"\"\n",
    "    sentence_transformed = transform_sentence(sentence, vocab)\n",
    "    for word_id in sentence_transformed:\n",
    "        fl.feature.add().int64_list.value.extend([word_id])\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.341026Z",
     "start_time": "2020-05-22T08:27:10.327456Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_example_train(row, vocab):\n",
    "    \"\"\"\n",
    "    Creates a training example for the Ubuntu Dialog Corpus dataset.\n",
    "    Returns a tensorflow Example Protocol Buffer object.\n",
    "    \"\"\"\n",
    "    context, utterance, label = row\n",
    "    context_transformed = transform_sentence(context, vocab)\n",
    "    utterance_transformed = transform_sentence(utterance, vocab)\n",
    "    context_len = len(next(vocab._tokenizer([context])))\n",
    "    utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "    label = int(float(label))\n",
    "\n",
    "    # New Example\n",
    "    example = tf.train.Example()\n",
    "    example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "    example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "    example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "    example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "    example.features.feature[\"label\"].int64_list.value.extend([label])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.377132Z",
     "start_time": "2020-05-22T08:27:10.346622Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_example_test(row, vocab):\n",
    "    \"\"\"\n",
    "    Creates a test/validation example for the Ubuntu Dialog Corpus dataset.  \n",
    "    Returns a tensorflow Example Protocol Buffer object.\n",
    "    \"\"\"\n",
    "    context, utterance = row[:2]\n",
    "    distractors = row[2:]\n",
    "    context_len = len(next(vocab._tokenizer([context])))\n",
    "    utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "    context_transformed = transform_sentence(context, vocab)\n",
    "    utterance_transformed = transform_sentence(utterance, vocab)\n",
    "    \n",
    "    # New Example\n",
    "    example = tf.train.Example()\n",
    "    example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "    example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "    example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "    example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "\n",
    "    # Distractor sequences\n",
    "    for i, distractor in enumerate(distractors):\n",
    "        dis_key = \"distractor_{}\".format(i)\n",
    "        dis_len_key = \"distractor_{}_len\".format(i)\n",
    "        # Distractor Length Feature\n",
    "        dis_len = len(next(vocab._tokenizer([distractor])))\n",
    "        example.features.feature[dis_len_key].int64_list.value.extend([dis_len])\n",
    "        # Distractor Text Feature\n",
    "        dis_transformed = transform_sentence(distractor, vocab)\n",
    "        example.features.feature[dis_key].int64_list.value.extend(dis_transformed)\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.396743Z",
     "start_time": "2020-05-22T08:27:10.381453Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_tfrecords_file(input_filename, output_filename, example_fn):\n",
    "    \"\"\"\n",
    "    Creates a TFRecords file for the given input data and\n",
    "    example transofmration function\n",
    "    \"\"\"\n",
    "    writer = tf.python_io.TFRecordWriter(output_filename)\n",
    "    print(\"Creating TFRecords file at {}...\".format(output_filename))\n",
    "    for i, row in enumerate(create_csv_iter(input_filename)):\n",
    "        x = example_fn(row)\n",
    "        writer.write(x.SerializeToString())\n",
    "    writer.close()\n",
    "    print(\"Wrote to {}\".format(output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.421250Z",
     "start_time": "2020-05-22T08:27:10.401141Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def write_vocabulary(vocab_processor, outfile):\n",
    "    \"\"\"\n",
    "    Writes the vocabulary to a file, one word per line.\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    with open(outfile, \"w\") as vocabfile:\n",
    "        for id in range(vocab_size):\n",
    "            word =  vocab_processor.vocabulary_._reverse_mapping[id]\n",
    "            vocabfile.write(word + \"\\n\")\n",
    "    print(\"Saved vocabulary to {}\".format(outfile))\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        print(\"Creating vocabulary...\")\n",
    "        input_iter = create_csv_iter(\"data/trainset.csv\")\n",
    "        input_iter = (x[0] + \" \" + x[1] for x in input_iter)\n",
    "        vocab = create_vocab(input_iter, min_frequency=FLAGS.min_word_frequency)\n",
    "        print(\"Total vocabulary size: {}\".format(len(vocab.vocabulary_)))\n",
    "\n",
    "    # Create vocabulary.txt file\n",
    "    write_vocabulary(\n",
    "        vocab, os.path.join(FLAGS.output_dir, \"vocabulary.txt\"))\n",
    "\n",
    "    # Save vocab processor\n",
    "    vocab.save(os.path.join(FLAGS.output_dir, \"vocab_processor.bin\"))\n",
    "\n",
    "    # Create validation.tfrecords\n",
    "    create_tfrecords_file(\n",
    "          input_filename=VALIDATION_PATH,\n",
    "          output_filename=os.path.join(FLAGS.output_dir, \"validation.tfrecords\"),\n",
    "          example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "    # Create test.tfrecords\n",
    "    create_tfrecords_file(\n",
    "          input_filename=TEST_PATH,\n",
    "          output_filename=os.path.join(FLAGS.output_dir, \"test.tfrecords\"),\n",
    "          example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "    # Create train.tfrecords\n",
    "    create_tfrecords_file(\n",
    "          input_filename=TRAIN_PATH,\n",
    "          output_filename=os.path.join(FLAGS.output_dir, \"train.tfrecords\"),\n",
    "          example_fn=functools.partial(create_example_train, vocab=vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.433282Z",
     "start_time": "2020-05-22T08:27:10.427340Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.446482Z",
     "start_time": "2020-05-22T08:27:10.439604Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_FEATURE_SIZE = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.471518Z",
     "start_time": "2020-05-22T08:27:10.452400Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_columns(mode):\n",
    "    feature_columns = []\n",
    "\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "        column_name=\"context\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "        column_name=\"context_len\", dimension=1, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "        column_name=\"utterance\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "    feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "        column_name=\"utterance_len\", dimension=1, dtype=tf.int64))\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "        # During training we have a label feature\n",
    "        feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "            column_name=\"label\", dimension=1, dtype=tf.int64))\n",
    "\n",
    "    if mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "        # During evaluation we have distractors\n",
    "        for i in range(9):\n",
    "            feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "                column_name=\"distractor_{}\".format(i), dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n",
    "            feature_columns.append(tf.contrib.layers.real_valued_column(\n",
    "                column_name=\"distractor_{}_len\".format(i), dimension=1, dtype=tf.int64))\n",
    "\n",
    "    return set(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:10.494418Z",
     "start_time": "2020-05-22T08:27:10.477706Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_input_fn(mode, input_files, batch_size, num_epochs):\n",
    "    \n",
    "    def input_fn():\n",
    "        \n",
    "        '''\n",
    "        Create a feature definition that describes the fields in our Example file\n",
    "        Read records from the input_files with tf.TFRecordReader\n",
    "        Parse the records according to the feature definition\n",
    "        Extract the training labels\n",
    "        Batch multiple examples and training labels\n",
    "        Return the batched examples and training labels\n",
    "        '''\n",
    "        \n",
    "        features = tf.contrib.layers.create_feature_spec_for_parsing(\n",
    "            get_feature_columns(mode))\n",
    "\n",
    "        feature_map = tf.contrib.learn.io.read_batch_features(\n",
    "            file_pattern=input_files,\n",
    "            batch_size=batch_size,\n",
    "            features=features,\n",
    "            reader=tf.TFRecordReader,\n",
    "            randomize_input=True,\n",
    "            num_epochs=num_epochs,\n",
    "            queue_capacity=200000 + batch_size * 10,\n",
    "            name=\"read_batch_features_{}\".format(mode))\n",
    "\n",
    "        # This is an ugly hack because of a current bug in tf.learn\n",
    "        # During evaluation TF tries to restore the epoch variable which isn't defined during training\n",
    "        # So we define the variable manually here\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            tf.get_variable(\n",
    "            \"read_batch_features_eval/file_name_queue/limit_epochs/epochs\",\n",
    "            initializer=tf.constant(0, dtype=tf.int64))\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            target = feature_map.pop(\"label\")\n",
    "        else:\n",
    "          # In evaluation we have 10 classes (utterances).\n",
    "          # The first one (index 0) is always the correct one\n",
    "            target = tf.zeros([batch_size, 1], dtype=tf.int64)\n",
    "        return feature_map, target #batched_features, labels \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.841733Z",
     "start_time": "2020-05-22T08:27:10.500497Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import functools\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec #(DEPRECATED)\n",
    "#from tensorflow.estimator.EstimatorSpec.eval_metric_ops import MetricSpec ## substitute would be fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.852966Z",
     "start_time": "2020-05-22T08:27:11.844079Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_evaluation_metrics():\n",
    "    '''\n",
    "    Functools.partial is to convert a function that takes 3 arguments \n",
    "    to one that only takes 2 arguments. \n",
    "    streaming_sparse_recall_at_k:\n",
    "        Streaming: metric is accumulated over multiple batches\n",
    "        Sparse: format of our labels.\n",
    "    '''\n",
    "    eval_metrics = {}\n",
    "    for k in [1, 2, 5, 10]:\n",
    "        eval_metrics[\"recall_at_%d\" % k] = MetricSpec(metric_fn=functools.partial(\n",
    "            tf.contrib.metrics.streaming_sparse_recall_at_k,\n",
    "            k=k))\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.883103Z",
     "start_time": "2020-05-22T08:27:11.858767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-2ac6bd471cd9>:13: MetricSpec.__init__ (from tensorflow.contrib.learn.python.learn.metric_spec) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.estimator.EstimatorSpec.eval_metric_ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'recall_at_1': <tensorflow.contrib.learn.python.learn.metric_spec.MetricSpec at 0x133dae650>,\n",
       " 'recall_at_2': <tensorflow.contrib.learn.python.learn.metric_spec.MetricSpec at 0x133dae890>,\n",
       " 'recall_at_5': <tensorflow.contrib.learn.python.learn.metric_spec.MetricSpec at 0x133daec90>,\n",
       " 'recall_at_10': <tensorflow.contrib.learn.python.learn.metric_spec.MetricSpec at 0x133daee10>}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_evaluation_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.891564Z",
     "start_time": "2020-05-22T08:27:11.886425Z"
    }
   },
   "outputs": [],
   "source": [
    "#Metric Spec se tiene que cambiar por Estimator? y ya?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom object that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.898662Z",
     "start_time": "2020-05-22T08:27:11.895394Z"
    }
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.915458Z",
     "start_time": "2020-05-22T08:27:11.902978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "tf.flags.DEFINE_integer(\n",
    "  \"vocab_size\",\n",
    "  91620,\n",
    "  \"The size of the vocabulary. Only change this if you changed the preprocessing\")\n",
    "\n",
    "# Model Parameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of the embeddings\")\n",
    "tf.flags.DEFINE_integer(\"rnn_dim\", 256, \"Dimensionality of the RNN cell\")\n",
    "tf.flags.DEFINE_integer(\"max_context_len\", 160, \"Truncate contexts to this length\")\n",
    "tf.flags.DEFINE_integer(\"max_utterance_len\", 80, \"Truncate utterance to this length\")\n",
    "\n",
    "# Pre-trained embeddings\n",
    "tf.flags.DEFINE_string(\"glove_path\", None, \"Path to pre-trained Glove vectors\")\n",
    "tf.flags.DEFINE_string(\"vocab_path\", None, \"Path to vocabulary.txt file\")\n",
    "\n",
    "# Training Parameters\n",
    "tf.compat.v1.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size during training\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\"eval_batch_size\", 16, \"Batch size during evaluation\")\n",
    "tf.compat.v1.flags.DEFINE_string(\"optimizer\", \"Adam\", \"Optimizer Name (Adam, Adagrad, etc)\")\n",
    "\n",
    "FLAGS = tf.compat.v1.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:27:11.923724Z",
     "start_time": "2020-05-22T08:27:11.918270Z"
    }
   },
   "outputs": [],
   "source": [
    "HParams = namedtuple(\n",
    "  \"HParams\",\n",
    "  [\n",
    "    \"batch_size\",\n",
    "    \"embedding_dim\",\n",
    "    \"eval_batch_size\",\n",
    "    \"learning_rate\",\n",
    "    \"max_context_len\",\n",
    "    \"max_utterance_len\",\n",
    "    \"optimizer\",\n",
    "    \"rnn_dim\",\n",
    "    \"vocab_size\",\n",
    "    \"glove_path\",\n",
    "    \"vocab_path\"\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:12.906516Z",
     "start_time": "2020-05-22T08:28:12.892389Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_hparams():\n",
    "    return HParams(\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        eval_batch_size=FLAGS.eval_batch_size,\n",
    "        vocab_size=FLAGS.vocab_size,\n",
    "        optimizer=FLAGS.optimizer,\n",
    "        learning_rate=FLAGS.learning_rate,\n",
    "        embedding_dim=FLAGS.embedding_dim,\n",
    "        max_context_len=FLAGS.max_context_len,\n",
    "        max_utterance_len=FLAGS.max_utterance_len,\n",
    "        glove_path=FLAGS.glove_path,\n",
    "        vocab_path=FLAGS.vocab_path,\n",
    "        rnn_dim=FLAGS.rnn_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:14.843647Z",
     "start_time": "2020-05-22T08:28:14.835533Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "# import udc_model\n",
    "# import udc_hparams\n",
    "# import udc_metrics\n",
    "# import udc_inputs\n",
    "#from models.dual_encoder import dual_encoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FLAGS**: a way to give command line parameters to the program (similar to Python’s argparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:16.251293Z",
     "start_time": "2020-05-22T08:28:16.199094Z"
    }
   },
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'input_dir' is defined twice. First from /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Input directory containing original CSV data files (default = './data')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0e7beb13d075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Directory containing input data files 'train.tfrecords' and 'validation.tfrecords'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Directory to store model checkpoints (defaults to ./runs)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loglevel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tensorflow log level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_epochs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Number of training Epochs. Defaults to indefinite.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_every\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Evaluate after this many train steps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'input_dir' is defined twice. First from /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py, Second from /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py.  Description from first occurrence: Input directory containing original CSV data files (default = './data')"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.flags.DEFINE_string(\"input_dir\", \"./data\", \"Directory containing input data files 'train.tfrecords' and 'validation.tfrecords'\")\n",
    "tf.compat.v1.flags.DEFINE_string(\"model_dir\", None, \"Directory to store model checkpoints (defaults to ./runs)\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\"loglevel\", 20, \"Tensorflow log level\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\"num_epochs\", None, \"Number of training Epochs. Defaults to indefinite.\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\"eval_every\", 2000, \"Evaluate after this many train steps\")\n",
    "\n",
    "FLAGS = tf.compat.v1.flags.FLAGS\n",
    "\n",
    "TIMESTAMP = int(time.time())\n",
    "\n",
    "if FLAGS.model_dir:\n",
    "    MODEL_DIR = FLAGS.model_dir\n",
    "else:\n",
    "    MODEL_DIR = os.path.abspath(os.path.join(\"./runs\", str(TIMESTAMP)))\n",
    "\n",
    "TRAIN_FILE = os.path.abspath(os.path.join(FLAGS.input_dir, \"train.tfrecords\"))\n",
    "VALIDATION_FILE = os.path.abspath(os.path.join(FLAGS.input_dir, \"validation.tfrecords\"))\n",
    "\n",
    "tf.logging.set_verbosity(FLAGS.loglevel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:20.279709Z",
     "start_time": "2020-05-22T08:28:20.268656Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    hparams = create_hparams()\n",
    "\n",
    "    model_fn = create_model_fn(\n",
    "        hparams,\n",
    "        model_impl=dual_encoder_model)\n",
    "\n",
    "    estimator = tf.contrib.learn.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        model_dir=MODEL_DIR,\n",
    "        config=tf.contrib.learn.RunConfig())\n",
    "\n",
    "    input_fn_train = create_input_fn(\n",
    "        mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "        input_files=[TRAIN_FILE],\n",
    "        batch_size=hparams.batch_size,\n",
    "        num_epochs=FLAGS.num_epochs)\n",
    "\n",
    "    input_fn_eval = create_input_fn(\n",
    "        mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "        input_files=[VALIDATION_FILE],\n",
    "        batch_size=hparams.eval_batch_size,\n",
    "        num_epochs=1)\n",
    "\n",
    "    eval_metrics = create_evaluation_metrics()\n",
    "\n",
    "    eval_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "            input_fn=input_fn_eval,\n",
    "            every_n_steps=FLAGS.eval_every,\n",
    "            metrics=eval_metrics)\n",
    "\n",
    "    estimator.fit(input_fn=input_fn_train, steps=None, monitors=[eval_monitor])\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "udc_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:21.510659Z",
     "start_time": "2020-05-22T08:28:21.507058Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:21.892848Z",
     "start_time": "2020-05-22T08:28:21.884144Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_id_feature(features, key, len_key, max_len):\n",
    "    ids = features[key]\n",
    "    ids_len = tf.squeeze(features[len_key], [1])\n",
    "    ids_len = tf.minimum(ids_len, tf.constant(max_len, dtype=tf.int64))\n",
    "    return ids, ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:22.356517Z",
     "start_time": "2020-05-22T08:28:22.349871Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_train_op(loss, hparams):\n",
    "    return tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        learning_rate=hparams.learning_rate,\n",
    "        clip_gradients=10.0,\n",
    "        optimizer=hparams.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:28:22.945865Z",
     "start_time": "2020-05-22T08:28:22.924315Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_fn(hparams, model_impl):\n",
    "\n",
    "    def model_fn(features, targets, mode):\n",
    "        context, context_len = get_id_feature(\n",
    "            features, \"context\", \"context_len\", hparams.max_context_len)\n",
    "\n",
    "        utterance, utterance_len = get_id_feature(\n",
    "            features, \"utterance\", \"utterance_len\", hparams.max_utterance_len)\n",
    "\n",
    "        batch_size = targets.get_shape().as_list()[0]\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            probs, loss = model_impl(\n",
    "              hparams,\n",
    "              mode,\n",
    "              context,\n",
    "              context_len,\n",
    "              utterance,\n",
    "              utterance_len,\n",
    "              targets)\n",
    "            train_op = create_train_op(loss, hparams)\n",
    "            return probs, loss, train_op\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            probs, loss = model_impl(\n",
    "              hparams,\n",
    "              mode,\n",
    "              context,\n",
    "              context_len,\n",
    "              utterance,\n",
    "              utterance_len,\n",
    "              None)\n",
    "            return probs, 0.0, None\n",
    "\n",
    "        if mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "\n",
    "            # We have 10 exampels per record, so we accumulate them\n",
    "            all_contexts = [context]\n",
    "            all_context_lens = [context_len]\n",
    "            all_utterances = [utterance]\n",
    "            all_utterance_lens = [utterance_len]\n",
    "            all_targets = [tf.ones([batch_size, 1], dtype=tf.int64)]\n",
    "\n",
    "        for i in range(9):\n",
    "            distractor, distractor_len = get_id_feature(features,\n",
    "                \"distractor_{}\".format(i),\n",
    "                \"distractor_{}_len\".format(i),\n",
    "                hparams.max_utterance_len)\n",
    "            all_contexts.append(context)\n",
    "            all_context_lens.append(context_len)\n",
    "            all_utterances.append(distractor)\n",
    "            all_utterance_lens.append(distractor_len)\n",
    "            all_targets.append(\n",
    "              tf.zeros([batch_size, 1], dtype=tf.int64)\n",
    "            )\n",
    "\n",
    "        probs, loss = model_impl(\n",
    "              hparams,\n",
    "              mode,\n",
    "              tf.concat(0, all_contexts),\n",
    "              tf.concat(0, all_context_lens),\n",
    "              tf.concat(0, all_utterances),\n",
    "              tf.concat(0, all_utterance_lens),\n",
    "              tf.concat(0, all_targets))\n",
    "\n",
    "        split_probs = tf.split(0, 10, probs)\n",
    "        shaped_probs = tf.concat(1, split_probs)\n",
    "\n",
    "        # Add summaries\n",
    "        tf.histogram_summary(\"eval_correct_probs_hist\", split_probs[0])\n",
    "        tf.scalar_summary(\"eval_correct_probs_average\", tf.reduce_mean(split_probs[0]))\n",
    "        tf.histogram_summary(\"eval_incorrect_probs_hist\", split_probs[1])\n",
    "        tf.scalar_summary(\"eval_incorrect_probs_average\", tf.reduce_mean(split_probs[1]))\n",
    "\n",
    "        return shaped_probs, loss, None\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:29:35.410744Z",
     "start_time": "2020-05-22T08:29:35.405778Z"
    }
   },
   "outputs": [],
   "source": [
    "model_fn = create_model_fn(hparams=create_hparams(), model_impl=dual_encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T08:29:54.110639Z",
     "start_time": "2020-05-22T08:29:54.086887Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_fn() missing 3 required positional arguments: 'features', 'targets', and 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-634bc825f8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: model_fn() missing 3 required positional arguments: 'features', 'targets', and 'mode'"
     ]
    }
   ],
   "source": [
    "model_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

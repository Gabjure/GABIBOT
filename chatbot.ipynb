{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:02.916308Z",
     "start_time": "2020-05-20T10:55:25.078116Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.13.0rc1 in /usr/local/lib/python3.7/site-packages (1.13.0rc1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.13.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.12.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (0.33.6)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.16.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (3.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (1.28.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.0rc1) (0.3.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.0rc1) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.13.0rc1) (41.2.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0rc1) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (0.23)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.7/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0rc1) (5.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0rc1) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "!pip3 install 'tensorflow==1.13.0rc1'\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "#data_processing\n",
    "import re\n",
    "\n",
    "#seq2seq\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:03.127643Z",
     "start_time": "2020-05-20T10:56:02.956040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:03.886836Z",
     "start_time": "2020-05-20T10:56:03.194446Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Main Data Pre-Processing File\n",
    "def create_data_from_files(lines_file, conversations_file, verbose=False):\n",
    "    ### Importing Dataset\n",
    "    with open(lines_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    with open(conversations_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        conversations = f.read().split('\\n')\n",
    "    if(verbose == True):\n",
    "        print('{} lines and {} conversations have been extracted.'.format(len(lines), len(conversations)))\n",
    "    return (lines, conversations)\n",
    "\n",
    "def create_lines_dictionary(lines, verbose=False):\n",
    "    '''\n",
    "    Creating a dictionary that maps each line and its id\n",
    "    '''\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line\n",
    "\n",
    "def create_conversations_ids(conversations, verbose=False):\n",
    "    '''\n",
    "    To get a list of conversation ids for each conversation by removing brackets, quotes and spaces.\n",
    "    '''\n",
    "    conversations_ids = []\n",
    "    for conversation in conversations[:-1]:\n",
    "        _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "        conversations_ids.append(_conversation.split(','))\n",
    "    if(verbose == True):\n",
    "        print('{} conversations ids have been created.'.format(len(conversations_ids)))\n",
    "    return conversations_ids\n",
    "\n",
    "#porque hace esto?\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def create_questions_and_answers(id2line, conversations_ids, verbose=False):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for conversation in conversations_ids:\n",
    "        for i in range(len(conversation) - 1):\n",
    "            questions.append(clean_text(id2line[conversation[i]]))\n",
    "            answers.append(clean_text(id2line[conversation[i+1]]))\n",
    "    if(verbose == True):\n",
    "        print('{} quesions and {} answers have been created.'.format(len(questions), len(answers)))\n",
    "    return (questions, answers)\n",
    "\n",
    "def get_words_and_occurences(questions, answers, verbose=False):\n",
    "    word2count = {}\n",
    "    for sentence in questions + answers:\n",
    "        for word in sentence.split():\n",
    "            if word in word2count:\n",
    "                word2count[word] += 1\n",
    "            else:\n",
    "                word2count[word] = 1\n",
    "    if(verbose == True):\n",
    "        print('{} words have been found.'.format(len(word2count)))\n",
    "    return word2count\n",
    "\n",
    "def remove_less_frequent_words(word2count, threshold, verbose=False):\n",
    "    questionswords2int = {}\n",
    "    answerswords2int = {}\n",
    "    word_number = 0\n",
    "    for word, count in word2count.items():\n",
    "        if count >= threshold:\n",
    "            questionswords2int[word] = word_number\n",
    "            answerswords2int[word] = word_number\n",
    "            word_number += 1\n",
    "    if(verbose == True):\n",
    "        print('Total tokens after removing less frequent words: ', len(answerswords2int))\n",
    "    return (questionswords2int, answerswords2int)\n",
    "\n",
    "def add_tokens_to_words(questionswords2int, answerswords2int, verbose=False):\n",
    "    tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "    for token in tokens:\n",
    "        questionswords2int[token] = len(questionswords2int) + 1\n",
    "        answerswords2int[token] = len(answerswords2int) + 1\n",
    "    if(verbose == True):\n",
    "        print('Total Tokens', len(answerswords2int))\n",
    "    return (questionswords2int, answerswords2int)\n",
    "\n",
    "def get_inverse_dictionary(answerswords2int, verbose=False):\n",
    "    answersints2word = {w_i:w for w, w_i in answerswords2int.items()}\n",
    "    if(verbose == True):\n",
    "        print('Words inverse dictionary of answers has been created!')\n",
    "    return answersints2word\n",
    "\n",
    "def add_eos_to_sentences(answers, verbose=False):\n",
    "    for i in range(len(answers)):\n",
    "        answers[i] += ' <EOS>'\n",
    "    if(verbose == True):\n",
    "        print('<EOS> has been added to answers')\n",
    "    return answers\n",
    "\n",
    "def words_to_tokens(questions, answers, questionswords2int, answerswords2int, verbose=False):\n",
    "    questions_to_int = []\n",
    "    for question in questions:\n",
    "        ints = []\n",
    "        for word in question.split():\n",
    "            if word not in questionswords2int:\n",
    "                ints.append(questionswords2int['<OUT>'])\n",
    "            else:\n",
    "                ints.append(questionswords2int[word])\n",
    "        questions_to_int.append(ints)\n",
    "    answers_to_int = []\n",
    "    for answer in answers:\n",
    "        ints = []\n",
    "        for word in answer.split():\n",
    "            if word not in answerswords2int:\n",
    "                ints.append(answerswords2int['<OUT>'])\n",
    "            else:\n",
    "                ints.append(answerswords2int[word])\n",
    "        answers_to_int.append(ints)\n",
    "    return (questions_to_int, answers_to_int)\n",
    "\n",
    "def sort_questions_and_answers(questions, answers, sequence_length, verbose=False):\n",
    "    sorted_questions = []\n",
    "    sorted_answers = []\n",
    "    for length in range(1, sequence_length + 1):\n",
    "        for i in enumerate(questions):\n",
    "            if(len(i[1]) == length):\n",
    "                sorted_questions.append(questions[i[0]])\n",
    "                sorted_answers.append(answers[i[0]])\n",
    "    if(verbose == True):\n",
    "        print('Questions and answers have been sorted according to the length of questions.')\n",
    "    return (sorted_questions, sorted_answers)\n",
    "\n",
    "def get_processed_questions_and_answers(lines_file, conversations_file, threshold, sequence_length, verbose=False):\n",
    "    lines, conversations = create_data_from_files(lines_file, conversations_file, verbose=verbose)\n",
    "    id2line = create_lines_dictionary(lines, verbose=verbose)\n",
    "    conversations_ids = create_conversations_ids(conversations, verbose=verbose)\n",
    "    questions, answers = create_questions_and_answers(id2line, conversations_ids, verbose=verbose)\n",
    "    word2count = get_words_and_occurences(questions, answers, verbose=verbose)\n",
    "    questionswords2int, answerswords2int = remove_less_frequent_words(word2count, threshold, verbose=verbose)\n",
    "    questionswords2int, answerswords2int = add_tokens_to_words(questionswords2int, answerswords2int, verbose=verbose)\n",
    "    answersints2word = get_inverse_dictionary(answerswords2int, verbose=verbose)\n",
    "    answers = add_eos_to_sentences(answers, verbose=verbose)\n",
    "    questions, answers = words_to_tokens(questions, answers, questionswords2int, answerswords2int, verbose=verbose)\n",
    "    questions, answers = sort_questions_and_answers(questions, answers, sequence_length, verbose=verbose)\n",
    "    return (questions, answers, questionswords2int, answerswords2int, answersints2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:04.116566Z",
     "start_time": "2020-05-20T10:56:03.920794Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Main utility functions\n",
    "def preprocess_targets(targets, words2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], words2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    return tf.concat([left_side, right_side], 1)\n",
    "\n",
    "def apply_padding(batch_of_sequences, words2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [words2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
    "\n",
    "def split_into_batches(questions, answers, questionswords2int, answerswords2int, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch\n",
    "\n",
    "def get_training_validation_data(questions, answers, validation_set_ratio):\n",
    "    training_validation_split = int(len(questions) * validation_set_ratio)\n",
    "    training_questions = questions[training_validation_split:]\n",
    "    training_answers = answers[training_validation_split:]\n",
    "    validation_questions = questions[:training_validation_split]\n",
    "    validation_answers = answers[:training_validation_split]\n",
    "    return (training_questions, training_answers, validation_questions, validation_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:04.573476Z",
     "start_time": "2020-05-20T10:56:04.182115Z"
    }
   },
   "outputs": [],
   "source": [
    "## Main encoding-decoding functions\n",
    "\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
    "                                                       cell_bw = encoder_cell,\n",
    "                                                       sequence_length = sequence_length,\n",
    "                                                       inputs = rnn_inputs,\n",
    "                                                       dtype = tf.float32)\n",
    "    return encoder_state\n",
    "\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length,\n",
    "                                      decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = \\\n",
    "                        tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                             attention_option = 'bahdanau',\n",
    "                                                             num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = 'attn_dec_train')\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = \\\n",
    "                                        tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                  training_decoder_function,\n",
    "                                                                  decoder_embedded_input,\n",
    "                                                                  sequence_length,\n",
    "                                                                  scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)\n",
    "\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length,\n",
    "                                        num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    '''\n",
    "    For decoding the test/validation set\n",
    "    '''\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = \\\n",
    "                                            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                                    attention_option = 'bahdanau',\n",
    "                                                                    num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = 'attn_dec_inf')\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = \\\n",
    "                                        tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                    test_decoder_function,\n",
    "                                                                    scope = decoding_scope)\n",
    "    return test_predictions\n",
    "\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words,\n",
    "                    sequence_length, rnn_size, num_layers, words2int, keep_prob, batch_size):\n",
    "    '''\n",
    "    Decoder RNN\n",
    "    '''\n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           words2int['<SOS>'],\n",
    "                                           words2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions\n",
    "\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words,\n",
    "                        encoder_embedding_size, decoder_embeddings_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedding_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                               answers_num_words + 1,\n",
    "                                                               encoder_embedding_size,\n",
    "                                                               initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedding_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embeddings_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T11:10:47.482729Z",
     "start_time": "2020-05-20T11:10:47.404218Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_arguments(args=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-v', '-V', '--verbose', action='store_true')\n",
    "    # Hyper Parameters\n",
    "    parser.add_argument('-bs', '--batch_size', type=int, default=32)\n",
    "    parser.add_argument('-th', '--threshold', type=int, default=20)\n",
    "    parser.add_argument('-sl', '--sequence_length', type=int, default=25)\n",
    "    parser.add_argument('-ep', '--epochs', type=int, default=100)\n",
    "    parser.add_argument('-rs', '--rnn_size', type=int, default=1024)\n",
    "    parser.add_argument('-nl', '--num_layers', type=int, default=3)\n",
    "    parser.add_argument('-ee', '--encoding_embedding_size', type=int, default=1024)\n",
    "    parser.add_argument('-de', '--decoding_embedding_size', type=int, default=1024)\n",
    "    parser.add_argument('-lr', '--learning_rate', type=float, default=0.001)\n",
    "    parser.add_argument('-lrd', '--learning_rate_decay', type=float, default=0.9)\n",
    "    parser.add_argument('-mlr', '--minimum_learning_rate', type=float, default=0.0001)\n",
    "    parser.add_argument('-kp', '--keep_probability', type=float, default=0.5)\n",
    "    parser.add_argument('-vr', '--validation_set_ratio', type=float, default=0.15)\n",
    "    # File locations\n",
    "    parser.add_argument('-lf', '--lines_file', required=False, default='./data/movie_lines.txt')\n",
    "    parser.add_argument('-cf', '--conversations_file', required=False, default='./data/movie_conversations.txt')\n",
    "    try:\n",
    "        arguments = parser.parse_args(args=args)\n",
    "    except:\n",
    "        parser.print_help()\n",
    "        sys.exit(0)\n",
    "    arguments = vars(arguments)\n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:04.849991Z",
     "start_time": "2020-05-20T10:56:04.820684Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_inputs(verbose = False):\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    if(verbose == True):\n",
    "        print('model inputs placeholders have been created!')\n",
    "    return (inputs, targets, lr, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:05.568576Z",
     "start_time": "2020-05-20T10:56:04.866210Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(batch_size = None,\n",
    "                threshold = None,\n",
    "                sequence_length = None,\n",
    "                epochs = None,\n",
    "                rnn_size = None,\n",
    "                num_layers = None,\n",
    "                encoding_embedding_size = None,\n",
    "                decoding_embedding_size = None,\n",
    "                learning_rate = None,\n",
    "                learning_rate_decay = None,\n",
    "                minimum_learning_rate = None,\n",
    "                keep_probability = None,\n",
    "                validation_set_ratio = None,\n",
    "                lines_file = None,\n",
    "                conversations_file = None,\n",
    "                verbose = None):\n",
    "    tf.reset_default_graph()\n",
    "    session = tf.InteractiveSession()\n",
    "    inputs, targets, lr, keep_prob = model_inputs(verbose)\n",
    "    questions, answers, questionswords2int, answerswords2int, answersints2word = \\\n",
    "                                                    get_processed_questions_and_answers(lines_file,\n",
    "                                                    conversations_file,\n",
    "                                                    threshold,\n",
    "                                                    sequence_length,\n",
    "                                                    verbose)\n",
    "    sequence_length = tf.placeholder_with_default(sequence_length, None, name = 'sequence_length')\n",
    "    input_shape = tf.shape(inputs)\n",
    "    print('Shape of inputs, targets after preprocess_targets: ', np.shape(inputs), np.shape(targets))\n",
    "    print('Shape of inputs, targets after preprocess_targets: ', np.shape(tf.reverse(inputs, [-1])), np.shape(targets))\n",
    "    training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                           targets,\n",
    "                                                           keep_prob,\n",
    "                                                           batch_size,\n",
    "                                                           sequence_length,\n",
    "                                                           len(answerswords2int),\n",
    "                                                           len(questionswords2int),\n",
    "                                                           encoding_embedding_size,\n",
    "                                                           decoding_embedding_size,\n",
    "                                                           rnn_size,\n",
    "                                                           num_layers,\n",
    "                                                           questionswords2int)\n",
    "    print('Shape of training_questions, test_predictions: ', np.shape(training_predictions), np.shape(test_predictions))\n",
    "    with tf.name_scope('optimization'):\n",
    "        print('Shape of training predictions, targets: ', np.shape(training_predictions), np.shape(targets))\n",
    "        loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                      targets,\n",
    "                                                      tf.ones([input_shape[0], sequence_length]))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients = optimizer.compute_gradients(loss_error)\n",
    "        print('Shape of gradients: ', np.shape(gradients))\n",
    "        clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "        optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)\n",
    "\n",
    "    # Training and Validation Split\n",
    "    training_questions, training_answers, validation_questions, validation_answers = \\\n",
    "                                get_training_validation_data(questions,\n",
    "                                                             answers,\n",
    "                                                             validation_set_ratio)\n",
    "    print('Shape of training: questions-> {}, answers->{}'.format(np.shape(training_questions), np.shape(training_answers)))\n",
    "    print('Shape of validation: questions-> {}, answers->{}'.format(np.shape(validation_questions), np.shape(validation_answers)))\n",
    "\n",
    "    # Training\n",
    "    batch_index_check_training_loss = 100\n",
    "    batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\n",
    "    total_training_loss_error = 0\n",
    "    list_validation_loss_error = []\n",
    "    early_stopping_check = 0\n",
    "    early_stopping_stop = 1000\n",
    "    checkpoint = \"chatbot_weight.ckpt\"\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Varibles have been initialized!')\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in \\\n",
    "                                enumerate(split_into_batches(training_questions,\n",
    "                                                             training_answers,\n",
    "                                                             questionswords2int,\n",
    "                                                             answerswords2int,\n",
    "                                                             batch_size)):\n",
    "            print('Shape of padded_question_in_batch, padded_answers_in_batch: ', np.shape(padded_questions_in_batch), np.shape(padded_answers_in_batch))\n",
    "            starting_time = time.time()\n",
    "            print('inputs: ', type(padded_questions_in_batch))\n",
    "            print('targets: ', type(padded_answers_in_batch))\n",
    "            print('learning_rate: ', type(learning_rate))\n",
    "            print('sequence_length: ', type(padded_questions_in_batch.shape[1]))\n",
    "            _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error],\n",
    "                                            feed_dict = {\n",
    "                                            inputs: padded_questions_in_batch,\n",
    "                                            targets: padded_answers_in_batch,\n",
    "                                            lr: learning_rate,\n",
    "                                            sequence_length: padded_questions_in_batch.shape[1],\n",
    "                                            keep_prob: keep_probability,\n",
    "                                            })\n",
    "            total_training_loss_error += batch_training_loss_error\n",
    "            ending_time  = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            if(batch_index % batch_index_check_training_loss == 0):\n",
    "                print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on {} Batches: {:d} seconds'.format(\n",
    "                                            epoch,\n",
    "                                            epochs,\n",
    "                                            batch_index,\n",
    "                                            len(training_questions) // batch_size,\n",
    "                                            batch_index_check_training_loss,\n",
    "                                            total_training_loss_error // batch_index_check_training_loss,\n",
    "                                            int(batch_index * batch_index_check_training_loss)\n",
    "                                            ))\n",
    "                total_training_loss_error = 0\n",
    "            if(batch_index % batch_index_check_validation_loss == 0 and batch_index > 0):\n",
    "                total_validation_loss_error = 0\n",
    "                starting_validation_time = time.time()\n",
    "                for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in \\\n",
    "                                                enumerate(split_into_batches(validation_questions,\n",
    "                                                                             validation_answers,\n",
    "                                                                             questionswords2int,\n",
    "                                                                             answerswords2int,\n",
    "                                                                             batch_size)):\n",
    "                    batch_validation_loss_error = session.run(loss_error,\n",
    "                                                    {\n",
    "                                                    inputs: padded_questions_in_batch,\n",
    "                                                    targets: padded_answers_in_batch,\n",
    "                                                    lr: learning_rate,\n",
    "                                                    sequence_length: padded_questions_in_batch.shape[1],\n",
    "                                                    keep_prob: 1\n",
    "                                                    })\n",
    "                    total_validation_loss_error += batch_validation_loss_error\n",
    "                    ending_validation_time  = time.time()\n",
    "                    batch_validation_time = endingvalidation__time - startingvalidation__time\n",
    "                    average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "                    print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_validation_time), ))\n",
    "                    learning_rate *= learning_rate_decay\n",
    "                    if learning_rate < minimum_learning_rate:\n",
    "                        learning_rate = minimum_learning_rate\n",
    "                    list_validation_loss_error.append(average_validation_loss_error)\n",
    "                    if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                        print('I speak better now')\n",
    "                        early_stopping_check = 0\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(session, checkpoint)\n",
    "                    else:\n",
    "                        print('Sorry, I do not speak better, I need to practice more!')\n",
    "                        early_stopping_check += 1\n",
    "                        if early_stopping_check >= early_stopping_stop:\n",
    "                            break\n",
    "        if early_stopping_check >= early_stopping_stop:\n",
    "            print(\"My apologies, I cannot speak better anymore, This is the best I can do!\")\n",
    "            break\n",
    "    print('Game over!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T10:56:05.623904Z",
     "start_time": "2020-05-20T10:56:05.600120Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_string2int(question, words2int):\n",
    "    question = clean_text(question)\n",
    "    return [words2int.get(word, words2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:42:06.089449Z",
     "start_time": "2020-05-20T15:42:06.034234Z"
    }
   },
   "outputs": [],
   "source": [
    "def chat_with_bot():\n",
    "    checkpoint = './checkpoint.ckpt'\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"You: \")\n",
    "        if(question == 'Goodbye'):\n",
    "            break\n",
    "        else:\n",
    "            question = convert_string2int(question, questionswords2int)\n",
    "            question = question + [questionswords2int['<PAD>']] * (20 - len(question))\n",
    "            fake_batch = np.zeros((batch_size, 20))\n",
    "            fake_batch[0] = question\n",
    "            predicted_answer = session.run(test_predictions, {input: fake_batch, keep_prob: 0.5})[0]\n",
    "            answer = ''\n",
    "            for i in np.argmax(predicted_answer, 1):\n",
    "                if answersints2word[i] == 'i':\n",
    "                    token = 'I'\n",
    "                elif answersints2word[i] == '<EOS>':\n",
    "                    token = '.'\n",
    "                elif answersints2word[i] == '<OUT>':\n",
    "                    token = 'out'\n",
    "                else:\n",
    "                    token = ' ' + answersints2word[i]\n",
    "                answer += token\n",
    "                if token == '.':\n",
    "                    break\n",
    "            print('Chatbot:', answer)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:42:06.089449Z",
     "start_time": "2020-05-20T15:42:06.034234Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(argv=sys.argv):\n",
    "    '''\n",
    "    The main implementation of the movie conversations chatbot\n",
    "    '''\n",
    "    arguments = parse_arguments(argv[1:])\n",
    "    build_model(**arguments)\n",
    "    chat_with_bot()\n",
    "    if __name__ == '__main__':\n",
    "        main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:43:15.954336Z",
     "start_time": "2020-05-20T15:42:55.211777Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: I\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'questionswords2int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-cb22442f6003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat_with_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-6e44a521bf59>\u001b[0m in \u001b[0;36mchat_with_bot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_string2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionswords2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestionswords2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfake_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'questionswords2int' is not defined"
     ]
    }
   ],
   "source": [
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
